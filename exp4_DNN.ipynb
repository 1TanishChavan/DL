{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function: Activation functions and their derivatives\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "# Helper function: Initialize parameters for a deep neural network\n",
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Helper function: Forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
    "        A = relu(Z)\n",
    "        caches.append((A_prev, Z, parameters['W' + str(l)], parameters['b' + str(l)]))\n",
    "\n",
    "    # Output layer\n",
    "    ZL = np.dot(parameters['W' + str(L)], A) + parameters['b' + str(L)]\n",
    "    AL = sigmoid(ZL)\n",
    "    caches.append((A, ZL, parameters['W' + str(L)], parameters['b' + str(L)]))\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "# Helper function: Compute the cost\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "# Helper function: Backward propagation\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)  # number of layers\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # Initial gradient on the output layer\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Output layer gradients\n",
    "    current_cache = caches[L - 1]\n",
    "    A_prev, ZL, W, b = current_cache\n",
    "    dZL = dAL * sigmoid_derivative(ZL)\n",
    "    grads[\"dW\" + str(L)] = (1/m) * np.dot(dZL, A_prev.T)\n",
    "    grads[\"db\" + str(L)] = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZL)\n",
    "\n",
    "    # Backpropagation for hidden layers\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        A_prev, Z, W, b = current_cache\n",
    "        dZ = dA_prev * relu_derivative(Z)\n",
    "        grads[\"dW\" + str(l + 1)] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        grads[\"db\" + str(l + 1)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Helper function: Update parameters using gradient descent\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Training the neural network\n",
    "def model(X, Y, layers_dims, learning_rate=0.01, num_iterations=10000):\n",
    "    np.random.seed(1)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Example: Train a 3-layer neural network (2 hidden layers)\n",
    "layers_dims = [3, 4, 4, 1]  # Input layer: 3 units, two hidden layers with 4 units each, output layer with 1 unit\n",
    "\n",
    "# Example input data (X) and labels (Y)\n",
    "X = np.random.randn(3, 5)  # 3 features, 5 examples\n",
    "Y = np.array([[1, 0, 1, 0, 1]])  # Corresponding labels\n",
    "\n",
    "# Train the model\n",
    "parameters = model(X, Y, layers_dims, learning_rate=0.01, num_iterations=10000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6715bf8f-7ea4-8008-9cfa-39a2c50c5503"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation Algorithm for Training a Deep Neural Network (DNN)\n",
    "\n",
    "**Introduction**:  \n",
    "Backpropagation is a key algorithm for training deep neural networks (DNNs), which allows the model to minimize its error by adjusting the network's weights and biases through gradient descent. A DNN consists of multiple layers, including an input layer, two or more hidden layers, and an output layer. Each layer applies an activation function to produce non-linear outputs, allowing the network to learn complex patterns.\n",
    "\n",
    "### Architecture:\n",
    "- **Input Layer**: This layer receives input features.\n",
    "- **Hidden Layers**: At least two hidden layers with non-linear activation functions like ReLU (Rectified Linear Unit) are used to introduce non-linearity into the model.\n",
    "- **Output Layer**: For classification tasks, the sigmoid function is often applied to the output layer to convert raw scores into probabilities.\n",
    "\n",
    "### Forward Propagation:\n",
    "1. **Initialization**: Weights (W) and biases (b) are randomly initialized.\n",
    "2. **Feedforward**: Input data is passed through each layer:\n",
    "   - Hidden layers apply the activation function (e.g., ReLU).\n",
    "   - The final output layer uses the sigmoid function to output probabilities for classification.\n",
    "   \n",
    "   \\[\n",
    "   Z = W \\cdot A_{\\text{prev}} + b\n",
    "   \\]\n",
    "   \\[\n",
    "   A = \\text{activation}(Z)\n",
    "   \\]\n",
    "\n",
    "### Cost Function:\n",
    "To measure the model's performance, a loss function (e.g., binary cross-entropy for classification) is computed after forward propagation:\n",
    "\\[\n",
    "\\text{Cost} = -\\frac{1}{m} \\sum \\left( Y \\cdot \\log(A) + (1-Y) \\cdot \\log(1-A) \\right)\n",
    "\\]\n",
    "where \\(Y\\) is the true label, \\(A\\) is the predicted output, and \\(m\\) is the number of training examples.\n",
    "\n",
    "### Backward Propagation:\n",
    "This is the key part of training a DNN. It computes the gradients of the cost function with respect to the network parameters (weights and biases) and updates them using gradient descent.\n",
    "1. **Calculate output layer gradients**:\n",
    "   \\[\n",
    "   dZ = A - Y\n",
    "   \\]\n",
    "   \\[\n",
    "   dW = \\frac{1}{m} \\cdot dZ \\cdot A_{\\text{prev}}^T\n",
    "   \\]\n",
    "   \\[\n",
    "   db = \\frac{1}{m} \\cdot \\sum dZ\n",
    "   \\]\n",
    "2. **Propagate backwards through the hidden layers**: Use the chain rule to compute gradients for hidden layers, applying the derivatives of the ReLU function.\n",
    "3. **Update Parameters**: Update weights and biases using the learning rate:\n",
    "   \\[\n",
    "   W = W - \\alpha \\cdot dW, \\quad b = b - \\alpha \\cdot db\n",
    "   \\]\n",
    "\n",
    "### Diagram:\n",
    "Hereâ€™s a simplified diagram of the process:\n",
    "\n",
    "```\n",
    "Input Layer --> [ W1, b1 ] --> Hidden Layer 1 (ReLU) --> [ W2, b2 ] --> Hidden Layer 2 (ReLU) --> [ W3, b3 ] --> Output Layer (Sigmoid) --> Prediction\n",
    "\n",
    "                        <-- Backpropagation (Calculate Gradients) <--                            \n",
    "```\n",
    "\n",
    "### Conclusion:\n",
    "The backpropagation algorithm, combined with gradient descent, iteratively reduces the error of a DNN by adjusting its weights and biases. By applying this technique, a DNN with at least two hidden layers can be trained effectively for classification tasks, with the network learning to map inputs to outputs through multiple layers of abstraction."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
