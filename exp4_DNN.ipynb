{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Helper function: Activation functions and their derivatives\n",
    "def sigmoid(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def sigmoid_derivative(Z):\n",
    "    return sigmoid(Z) * (1 - sigmoid(Z))\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "# Helper function: Initialize parameters for a deep neural network\n",
    "def initialize_parameters(layers_dims):\n",
    "    np.random.seed(1)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l - 1]) * 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Helper function: Forward propagation\n",
    "def forward_propagation(X, parameters):\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        A_prev = A\n",
    "        Z = np.dot(parameters['W' + str(l)], A_prev) + parameters['b' + str(l)]\n",
    "        A = relu(Z)\n",
    "        caches.append((A_prev, Z, parameters['W' + str(l)], parameters['b' + str(l)]))\n",
    "\n",
    "    # Output layer\n",
    "    ZL = np.dot(parameters['W' + str(L)], A) + parameters['b' + str(L)]\n",
    "    AL = sigmoid(ZL)\n",
    "    caches.append((A, ZL, parameters['W' + str(L)], parameters['b' + str(L)]))\n",
    "\n",
    "    return AL, caches\n",
    "\n",
    "# Helper function: Compute the cost\n",
    "def compute_cost(AL, Y):\n",
    "    m = Y.shape[1]\n",
    "    cost = -(1/m) * np.sum(Y * np.log(AL) + (1 - Y) * np.log(1 - AL))\n",
    "    return np.squeeze(cost)\n",
    "\n",
    "# Helper function: Backward propagation\n",
    "def backward_propagation(AL, Y, caches):\n",
    "    grads = {}\n",
    "    L = len(caches)  # number of layers\n",
    "    m = AL.shape[1]\n",
    "\n",
    "    # Initial gradient on the output layer\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "\n",
    "    # Output layer gradients\n",
    "    current_cache = caches[L - 1]\n",
    "    A_prev, ZL, W, b = current_cache\n",
    "    dZL = dAL * sigmoid_derivative(ZL)\n",
    "    grads[\"dW\" + str(L)] = (1/m) * np.dot(dZL, A_prev.T)\n",
    "    grads[\"db\" + str(L)] = (1/m) * np.sum(dZL, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZL)\n",
    "\n",
    "    # Backpropagation for hidden layers\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        A_prev, Z, W, b = current_cache\n",
    "        dZ = dA_prev * relu_derivative(Z)\n",
    "        grads[\"dW\" + str(l + 1)] = (1/m) * np.dot(dZ, A_prev.T)\n",
    "        grads[\"db\" + str(l + 1)] = (1/m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = np.dot(W.T, dZ)\n",
    "\n",
    "    return grads\n",
    "\n",
    "# Helper function: Update parameters using gradient descent\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "\n",
    "    for l in range(1, L + 1):\n",
    "        parameters[\"W\" + str(l)] -= learning_rate * grads[\"dW\" + str(l)]\n",
    "        parameters[\"b\" + str(l)] -= learning_rate * grads[\"db\" + str(l)]\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Training the neural network\n",
    "def model(X, Y, layers_dims, learning_rate=0.01, num_iterations=10000):\n",
    "    np.random.seed(1)\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation\n",
    "        AL, caches = forward_propagation(X, parameters)\n",
    "\n",
    "        # Compute cost\n",
    "        cost = compute_cost(AL, Y)\n",
    "\n",
    "        # Backward propagation\n",
    "        grads = backward_propagation(AL, Y, caches)\n",
    "\n",
    "        # Update parameters\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "\n",
    "    return parameters\n",
    "\n",
    "# Example: Train a 3-layer neural network (2 hidden layers)\n",
    "layers_dims = [3, 4, 4, 1]  # Input layer: 3 units, two hidden layers with 4 units each, output layer with 1 unit\n",
    "\n",
    "# Example input data (X) and labels (Y)\n",
    "X = np.random.randn(3, 5)  # 3 features, 5 examples\n",
    "Y = np.array([[1, 0, 1, 0, 1]])  # Corresponding labels\n",
    "\n",
    "# Train the model\n",
    "parameters = model(X, Y, layers_dims, learning_rate=0.01, num_iterations=10000)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
