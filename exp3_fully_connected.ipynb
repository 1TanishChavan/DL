{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Encode the target labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(y)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the DNN model\n",
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_dim=X.shape[1]))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))  # 3 output classes for Iris species\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',  # Use sparse categorical crossentropy for integer labels\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test loss: {loss:.4f}, Test accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Neural Network for Classification\n",
    "\n",
    "A **Deep Neural Network (DNN)** is an artificial neural network with multiple layers between the input and output layers. It can model complex relationships between features (input data) and their corresponding labels (output data). In this practical, we design and implement a fully connected DNN with at least two hidden layers for a classification task.\n",
    "\n",
    "#### Components of the Deep Neural Network\n",
    "\n",
    "1. **Fully Connected (Dense) Layers**: \n",
    "   - Each neuron in one layer is connected to every neuron in the next layer. In the case of a classification problem, the network takes in input features, passes them through multiple hidden layers, and produces a class prediction at the output layer.\n",
    "\n",
    "2. **Input Layer**: \n",
    "   - This layer receives the input data. For instance, if we're working with a dataset like MNIST (handwritten digits), the input layer would accept pixel values for an image.\n",
    "\n",
    "3. **Hidden Layers**: \n",
    "   - The network contains at least two hidden layers. These layers perform transformations on the input data using activation functions like ReLU (Rectified Linear Unit). The deeper the network, the more abstract representations it can learn.\n",
    "   - Example architecture: \n",
    "     - **1st hidden layer**: 128 neurons, activation function: ReLU.\n",
    "     - **2nd hidden layer**: 64 neurons, activation function: ReLU.\n",
    "\n",
    "4. **Output Layer**: \n",
    "   - The output layer depends on the number of classes in the classification task. If there are three classes (e.g., classifying between three types of flowers), the output layer will have three neurons, each representing one class.\n",
    "   - The **softmax** activation function is commonly used for the output layer in multi-class classification tasks. It converts the network output into a probability distribution over all classes.\n",
    "\n",
    "#### Learning Algorithm\n",
    "\n",
    "- **Gradient Descent Optimization**: \n",
    "   - The network is trained using optimization algorithms like **Adam** (Adaptive Moment Estimation), a variant of gradient descent. Adam updates the network weights by minimizing the loss over multiple iterations, based on the error between predicted and actual labels.\n",
    "\n",
    "#### Loss Function\n",
    "\n",
    "- **Loss function**: \n",
    "   - For classification tasks, the **categorical cross-entropy** (or **sparse categorical cross-entropy** if the labels are integer-encoded) is a suitable choice. This function computes the difference between the true class labels and the predicted probability distribution generated by the network.\n",
    "\n",
    "#### Diagram\n",
    "\n",
    "Here is a basic diagram of a fully connected deep neural network with two hidden layers:\n",
    "\n",
    "```\n",
    "Input Layer (features) --> Hidden Layer 1 (ReLU) --> Hidden Layer 2 (ReLU) --> Output Layer (Softmax)\n",
    "```\n",
    "\n",
    "- **Input Layer**: Takes input features (e.g., 4 features for the Iris dataset).\n",
    "- **Hidden Layer 1**: Applies transformations to learn internal representations.\n",
    "- **Hidden Layer 2**: Further refines learned features.\n",
    "- **Output Layer**: Produces the probability distribution over the classes.\n",
    "\n",
    "#### Workflow Summary\n",
    "1. **Data Preprocessing**: \n",
    "   - Load and preprocess the dataset (e.g., normalization, label encoding).\n",
    "   \n",
    "2. **Model Design**: \n",
    "   - Define the fully connected neural network with at least two hidden layers.\n",
    "   \n",
    "3. **Compilation**: \n",
    "   - Choose the optimizer (Adam), the loss function (categorical cross-entropy), and accuracy as the evaluation metric.\n",
    "   \n",
    "4. **Training**: \n",
    "   - Train the model by feeding the data through the network and adjusting weights using backpropagation.\n",
    "   \n",
    "5. **Evaluation**: \n",
    "   - Evaluate the model's performance on unseen test data using accuracy.\n",
    "\n",
    "This approach efficiently classifies data with the help of deep learning techniques, making it a robust solution for various classification problems.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6715bf62-120c-8008-93af-9176466326ca"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
