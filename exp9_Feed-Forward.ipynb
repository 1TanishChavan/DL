{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "# Load the MNIST dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# Preprocess the data: normalize and one-hot encode labels\n",
    "X_train = X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "# Build a simple feed-forward neural network\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),  # Flatten 28x28 images to a 1D vector\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')  # Output layer with 10 classes\n",
    "])\n",
    "# Compile the model using SGD\n",
    "model.compile(optimizer=SGD(learning_rate=0.01),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "# Train the model and store history\n",
    "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10, batch_size=32)\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(12, 5))\n",
    "# Subplot 1: Loss over time\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Subplot 2: Accuracy over time\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "# Show the plots\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-Forward Neural Networks for Classification of Temporal Data\n",
    "\n",
    "Feed-forward neural networks (FNNs) are one of the simplest forms of artificial neural networks (ANNs) and are widely used for supervised learning tasks, including classification. While FNNs are typically used for static data, they can also be adapted for temporal data by flattening or encoding the time-based sequences into static formats.\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Feed-Forward Neural Network (FNN)**:\n",
    "   - In an FNN, information flows in one directionâ€”from input nodes through hidden layers to output nodes. There are no cycles or loops, hence the term \"feed-forward.\" \n",
    "   - Each layer consists of neurons that are connected to the neurons in the next layer. Neurons process inputs using weighted sums followed by activation functions (like ReLU or sigmoid).\n",
    "   \n",
    "2. **Temporal Data**:\n",
    "   - Temporal data refers to data where the ordering of observations over time matters. Examples include time-series data, sensor readings, and sequences of events.\n",
    "   - To apply FNNs to temporal data, sequences are often flattened or transformed into fixed-size vectors, which can limit the model's ability to capture long-term dependencies unless further techniques are applied.\n",
    "\n",
    "#### Architecture of FNN for Temporal Data Classification\n",
    "\n",
    "In a typical architecture for classifying temporal data using a feed-forward network:\n",
    "- **Input Layer**: \n",
    "  - Temporal data (e.g., a sequence of values) is often flattened into a single feature vector. For instance, a time-series of 100 time steps may be converted into a vector of length 100.\n",
    "  - Feature extraction techniques (e.g., using summary statistics) can also be used to reduce the dimensionality of the input.\n",
    "  \n",
    "- **Hidden Layers**:\n",
    "  - Each hidden layer consists of multiple neurons that apply linear transformations followed by activation functions (such as ReLU or tanh). These layers learn hierarchical representations of the data.\n",
    "  \n",
    "- **Output Layer**:\n",
    "  - The output layer typically uses a softmax activation function for classification tasks, generating probabilities for each possible class.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "- The model is trained using labeled temporal data. Each training example consists of a sequence of input features and the corresponding class label.\n",
    "- The loss function, typically **categorical cross-entropy** for classification tasks, measures the difference between the predicted class distribution and the actual labels.\n",
    "- **Backpropagation** is used to update the weights in the network by minimizing the loss function using gradient-based optimization techniques such as **Stochastic Gradient Descent (SGD)** or **Adam**.\n",
    "\n",
    "#### Diagram of Feed-Forward Neural Network for Temporal Data Classification\n",
    "\n",
    "Below is a simplified diagram representing the architecture of a feed-forward neural network for temporal data classification.\n",
    "\n",
    "```\n",
    "    Input Layer (Flattened Temporal Data)\n",
    "    ------------------------------------\n",
    "    |    X1  X2  X3  ...  Xn            |\n",
    "    ------------------------------------\n",
    "                 |\n",
    "                 v\n",
    "    -------------------------\n",
    "    |     Hidden Layer 1     |\n",
    "    -------------------------\n",
    "                 |\n",
    "                 v\n",
    "    -------------------------\n",
    "    |     Hidden Layer 2     |\n",
    "    -------------------------\n",
    "                 |\n",
    "                 v\n",
    "    -------------------------\n",
    "    |     Output Layer       |\n",
    "    |   (Softmax for Class)  |\n",
    "    -------------------------\n",
    "```\n",
    "\n",
    "### Application\n",
    "\n",
    "Feed-forward neural networks are used for tasks such as:\n",
    "- Time-series classification (e.g., predicting stock prices, classifying speech segments)\n",
    "- Activity recognition (e.g., detecting movement patterns from sensor data)\n",
    "\n",
    "While FNNs work for temporal data by flattening sequences, more advanced architectures like **Recurrent Neural Networks (RNNs)** or **Long Short-Term Memory (LSTM)** networks are often preferred when it is critical to capture temporal dependencies.\n",
    "\n",
    "#### Limitations\n",
    "\n",
    "FNNs do not inherently capture temporal dependencies because they do not have a memory of past inputs. For true sequence modeling, specialized architectures like RNNs or LSTMs, which are designed to handle sequential data, would perform better.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6715bfca-4530-800d-9c57-ad5f7df2895d"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
