{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the sigmoid activation function\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Define the derivative of the sigmoid function\n",
    "def sigmoid_derivative(x):\n",
    "  return x * (1 - x)\n",
    "\n",
    "# Input dataset\n",
    "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "\n",
    "# Output dataset\n",
    "y = np.array([[0], [1], [1], [0]])\n",
    "\n",
    "# Define the number of neurons in each layer\n",
    "input_layer_neurons = 2\n",
    "hidden_layer_neurons = 2\n",
    "output_layer_neurons = 1\n",
    "\n",
    "# Initialize weights and biases with random values\n",
    "hidden_weights = np.random.uniform(size=(input_layer_neurons, hidden_layer_neurons))\n",
    "hidden_bias = np.random.uniform(size=(1, hidden_layer_neurons))\n",
    "output_weights = np.random.uniform(size=(hidden_layer_neurons, output_layer_neurons))\n",
    "output_bias = np.random.uniform(size=(1, output_layer_neurons))\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "\n",
    "# Training the model\n",
    "for epoch in range(epochs):\n",
    "  # Forward propagation\n",
    "  hidden_layer_activation = np.dot(X, hidden_weights) + hidden_bias\n",
    "  hidden_layer_output = sigmoid(hidden_layer_activation)\n",
    "  output_layer_activation = np.dot(hidden_layer_output, output_weights) + output_bias\n",
    "  predicted_output = sigmoid(output_layer_activation)\n",
    "\n",
    "  # Backpropagation\n",
    "  error = y - predicted_output\n",
    "  d_predicted_output = error * sigmoid_derivative(predicted_output)\n",
    "  error_hidden_layer = d_predicted_output.dot(output_weights.T)\n",
    "  d_hidden_layer = error_hidden_layer * sigmoid_derivative(hidden_layer_output)\n",
    "\n",
    "  # Update weights and biases\n",
    "  output_weights += hidden_layer_output.T.dot(d_predicted_output) * learning_rate\n",
    "  output_bias += np.sum(d_predicted_output, axis=0, keepdims=True) * learning_rate\n",
    "  hidden_weights += X.T.dot(d_hidden_layer) * learning_rate\n",
    "  hidden_bias += np.sum(d_hidden_layer, axis=0, keepdims=True) * learning_rate\n",
    "\n",
    "# Test the model\n",
    "print(\"Input: \\n\" + str(X))\n",
    "print(\"Actual Output: \\n\" + str(y))\n",
    "print(\"Predicted Output: \\n\", predicted_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron (MLP) to Simulate XOR Gate\n",
    "\n",
    "A **Multilayer Perceptron (MLP)** is a type of artificial neural network (ANN) composed of multiple layers of neurons. It includes an input layer, one or more hidden layers, and an output layer. Each layer is fully connected to the next, and information flows in one direction â€” from input to output. MLPs are well-suited for solving problems that are not linearly separable, like the XOR problem.\n",
    "\n",
    "#### XOR Gate and Its Non-Linearity\n",
    "\n",
    "The **XOR (Exclusive OR)** gate is a logical gate that outputs true (1) when the inputs are different, and false (0) when the inputs are the same. The XOR function is **non-linearly separable**, meaning it cannot be separated using a straight line in a 2D plane. This makes XOR a classic example used to demonstrate the power of multilayer neural networks, as simple perceptrons cannot solve it.\n",
    "\n",
    "| Input A | Input B | XOR Output |\n",
    "|---------|---------|------------|\n",
    "|    0    |    0    |      0     |\n",
    "|    0    |    1    |      1     |\n",
    "|    1    |    0    |      1     |\n",
    "|    1    |    1    |      0     |\n",
    "\n",
    "#### Architecture of MLP for XOR\n",
    "The MLP used to simulate the XOR gate consists of:\n",
    "1. **Input Layer**: 2 neurons, corresponding to the two inputs (A, B).\n",
    "2. **Hidden Layer**: 2 neurons to capture the non-linearity of the XOR function.\n",
    "3. **Output Layer**: 1 neuron to produce the XOR result.\n",
    "\n",
    "#### Steps in MLP for XOR Simulation\n",
    "\n",
    "1. **Initialization**:\n",
    "   - The weights for both the hidden and output layers are initialized randomly. Biases are also added to ensure flexibility in learning.\n",
    "\n",
    "2. **Forward Propagation**:\n",
    "   - Inputs are fed into the network, and the weighted sums of inputs plus biases are computed for each neuron in the hidden layer.\n",
    "   - A non-linear activation function like **sigmoid** is applied to these sums, producing outputs for the hidden layer.\n",
    "   - These hidden layer outputs are passed to the output layer, where another weighted sum and sigmoid activation produce the final output.\n",
    "\n",
    "3. **Backpropagation**:\n",
    "   - The network compares the predicted output with the actual XOR output and computes the error.\n",
    "   - Using this error, the network performs backpropagation to adjust the weights. The **gradient descent** algorithm minimizes the error by updating weights and biases based on the calculated gradients.\n",
    "\n",
    "4. **Training**:\n",
    "   - The network repeats forward and backpropagation over multiple **epochs** (iterations) to reduce the error. The learning rate controls how much the weights are adjusted in each step.\n",
    "\n",
    "After sufficient training, the network can simulate the XOR gate, producing outputs close to the expected XOR values for any binary input pair.\n",
    "\n",
    "#### Diagram of MLP for XOR Gate\n",
    "Below is a simplified diagram of the MLP architecture used for the XOR gate:\n",
    "\n",
    "```\n",
    "   Input Layer       Hidden Layer      Output Layer\n",
    "   (A, B)          (H1, H2)          (Output)\n",
    "    0 ----(w1)----> H1 ----(w5)---->  XOR\n",
    "    1 ----(w2)----> H1 ----(w6)---->  XOR\n",
    "    0 ----(w3)----> H2 ----(w7)---->  XOR\n",
    "    1 ----(w4)----> H2 ----(w8)---->  XOR\n",
    "```\n",
    "\n",
    "Where `w1, w2, w3...` represent weights between the layers, and the neurons in the hidden layer (H1, H2) apply the activation function to learn non-linearities. The XOR output is computed based on these transformations.\n",
    "\n",
    "### Conclusion\n",
    "The XOR problem showcases the strength of MLPs in solving non-linearly separable problems. By using a hidden layer with non-linear activation, the MLP can effectively learn and simulate the behavior of an XOR gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6715bf48-48a4-8008-915e-c0b38ce59d6f"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
