{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "# Load the IMDb dataset\n",
    "max_features = 10000  # Use only the top 10,000 words\n",
    "max_len = 500         # Cut off reviews after 500 words\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "# Pad sequences to ensure they have the same length\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "print(f'x_train shape: {x_train.shape}, y_train shape: {y_train.shape}')\n",
    "print(f'x_test shape: {x_test.shape}, y_test shape: {y_test.shape}')\n",
    "# Build the RNN model\n",
    "model = Sequential()\n",
    "\n",
    "# Embedding layer: Maps each word index to a vector of size 32\n",
    "model.add(Embedding(max_features, 32, input_length=max_len))\n",
    "\n",
    "# SimpleRNN layer\n",
    "model.add(SimpleRNN(32, return_sequences=False))  # return_sequences=False for classification\n",
    "\n",
    "# Dense output layer with a sigmoid activation for binary classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "# Train the model\n",
    "batch_size = 64\n",
    "epochs = 5\n",
    "\n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_data=(x_test, y_test))\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(f'Test Accuracy: {test_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical: Design and Implement RNN for Classification of Temporal Data and Sequence-to-Sequence Modeling\n",
    "\n",
    "**Introduction to RNN**  \n",
    "Recurrent Neural Networks (RNNs) are a class of neural networks specifically designed to handle sequential data, where the order of the data points is essential. Temporal data, such as time-series, speech, or text, require the model to learn patterns and dependencies across sequences, making RNNs ideal for tasks like classification of sequential data and sequence-to-sequence modeling.\n",
    "\n",
    "Unlike traditional feedforward neural networks, RNNs maintain a hidden state (memory) that captures information about previous time steps. This enables them to learn dependencies in data over time.\n",
    "\n",
    "---\n",
    "\n",
    "**Architecture of RNN**  \n",
    "The key feature of an RNN is the presence of a recurrent connection that loops information from the previous time step back into the model. The recurrent layer updates its hidden state \\(h_t\\) at each time step \\(t\\) using the current input \\(x_t\\) and the previous hidden state \\(h_{t-1}\\).\n",
    "\n",
    "The formula governing an RNN is:\n",
    "\n",
    "\\[\n",
    "h_t = f(W_{xh}x_t + W_{hh}h_{t-1} + b_h)\n",
    "\\]\n",
    "where \\(W_{xh}\\) is the weight matrix between input and hidden state, \\(W_{hh}\\) is the weight matrix between hidden states, \\(b_h\\) is the bias, and \\(f\\) is a non-linear activation function (often \\(tanh\\) or \\(ReLU\\)).\n",
    "\n",
    "The output \\(y_t\\) is computed as:\n",
    "\n",
    "\\[\n",
    "y_t = f(W_{hy}h_t + b_y)\n",
    "\\]\n",
    "where \\(W_{hy}\\) is the weight matrix between hidden state and output, and \\(b_y\\) is the bias for the output.\n",
    "\n",
    "---\n",
    "\n",
    "**Applications**  \n",
    "1. **Classification of Temporal Data**: RNNs can classify sequences such as sentiment analysis on movie reviews or speech recognition tasks, where each word or time step affects the overall classification.\n",
    "2. **Sequence-to-Sequence Modeling**: RNNs can generate an output sequence from an input sequence. For example, in machine translation, an RNN can translate a sentence from one language to another by learning to output each word in the correct order.\n",
    "\n",
    "---\n",
    "\n",
    "**Diagram of RNN Structure**\n",
    "\n",
    "```\n",
    "Input Sequence:  x1 → x2 → x3 → ... → xn\n",
    "                     ↓    ↓    ↓         ↓\n",
    "                  [RNN] [RNN] [RNN] ... [RNN]\n",
    "                     ↓    ↓    ↓         ↓\n",
    "               Output: y1   y2   y3 ...   yn\n",
    "```\n",
    "\n",
    "Each input \\(x_t\\) at time step \\(t\\) passes through an RNN cell, which produces both an output \\(y_t\\) and carries forward the hidden state to the next time step.\n",
    "\n",
    "---\n",
    "\n",
    "**Advantages of RNN**  \n",
    "- **Temporal dependencies**: RNNs are well-suited for tasks that require learning relationships over time.\n",
    "- **Memory**: The hidden state allows RNNs to store information from previous time steps.\n",
    "  \n",
    "**Limitations of RNN**  \n",
    "- **Vanishing Gradient Problem**: Standard RNNs struggle with long sequences because the gradient may become too small to update weights effectively during backpropagation.\n",
    "- **Training Complexity**: RNNs are harder to train compared to other architectures like Convolutional Neural Networks (CNNs).\n",
    "\n",
    "---\n",
    "\n",
    "**Conclusion**  \n",
    "RNNs are powerful tools for handling sequential data and temporal dependencies. They are widely used for tasks like sequence classification and sequence generation. However, their effectiveness can be limited by issues such as vanishing gradients, which can be addressed using advanced versions like LSTMs and GRUs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/6715bec4-87cc-800d-9945-f9f449156575"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
